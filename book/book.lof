\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Connection between research aims (left) and research problems (right)}}{3}{figure.1.1}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Research concept of dimensional speech emotion recognition by fusing acoustic and linguistic information}}{4}{figure.1.2}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Organization of the dissertation}}{6}{figure.1.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Plutchik wheel of emotions}}{9}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Graphical representation of circumplex model (VA space)\cite {Posner2005}; vertical axis: arousal; horizontal axis: valence}}{10}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Division of acoustic features for SER}}{11}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Divisions of linguistic features used in SER}}{14}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Graphical illustration of LSTM \cite {Zhang2020}}}{17}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Different scheme of fusing acoustic with linguistic information; (a), (b), (c): early fusion approach; (d): late fusion approach}}{18}{figure.2.6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The DIK hierarchy and its representation in speech emotion recognition; information (I) is extracted from data (D); knowledge (K) is extracted from information.}}{24}{figure.3.1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Hann window and its spectrum}}{30}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces An example of Hamming window (middle) applied to sinusoid signal (left); the resulted windowed signal (right) is multiplication of both.}}{30}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Frame-based processing for extracting low-level descriptors of an acoustic signal; the signal is an excerpt of IEMOCAP utterance with 400 samples frame length and 160 samples hop length; sampling frequency is 16 kHz.}}{31}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of MFCC features with 13 coefficients (top), mel-spectrogram (middle), and log mel-spectrogram with 64 mels (bottom)}}{33}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Illustration of Mean+Std extraction from LLDs (e.g., MFCCs)}}{36}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Calculation of silent region in speech}}{39}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Silent pause features calculation}}{42}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Different silent threshold factors on normalized RMS with trimmed leading and trailing silences}}{42}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Flow diagram of acoustic input feature aggregation}}{45}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Flow diagram of acoustic output aggregation (majority voting)}}{45}{figure.4.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Two architectures of word2vec: (a) CBOW and (b) skip-gram \cite {Mikolov}}}{49}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Illustration of GloVe representation}}{50}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Surface plot of different $\alpha $ and $\beta $ factors for MTL with two parameters; the best mean CCC score of 0.51 was obtained using $\alpha =0.7$ and $\beta =0.2$; both factors were searched simultaneously/dependently.}}{56}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces CCC scores for MTL with three parameters, obtained to find the optimal weighting factors; a linear search was performed independently on each parameter; the best weighting factors for the three parameters were $\alpha =0.9$, $\beta =0.9$ and $\gamma =0.2$.}}{57}{figure.5.4}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Analysis of dropout rates applied to the acoustic and linguistic networks before concatenating them; the dropout rates were applied independently to either network while keeping a fixed rate for the other network.}}{58}{figure.5.5}
\contentsline {figure}{\numberline {5.6}{\ignorespaces SER architecture by fusing acoustic and linguistic features from ASR outputs}}{59}{figure.5.6}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Acoustic-linguistic feature concatenation with SVM}}{61}{figure.5.7}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Proportion of data splitting for each partition of each dataset. In one-stage LSTM processing, the outputs of the model are both development and test data. In the second stage, i.e., the SVM processing, the input data is the prediction from the development set of the previous stage, and the output is the prediction of test data.}}{65}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Structure of acoustic network to process acoustic features}}{67}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Structure of linguistic network to process word embeddings/vectors}}{69}{figure.6.3}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Proposed two-stage dimensional emotion recognition method using DNNs and an SVM. The inputs are acoustic features (af) and linguistic features (lf); the outputs are valence (V), arousal (A), and dominance (D).}}{70}{figure.6.4}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Relative improvements in average CCC scores from the late fusion using an SVM as compared to the highest average CCC scores from a single modality}}{75}{figure.6.5}
\addvspace {10\p@ }
\addvspace {10\p@ }
